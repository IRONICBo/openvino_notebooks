{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d7e4eb",
   "metadata": {},
   "source": [
    "# Vehicle Detection And Recognition with OpenVINO\n",
    "\n",
    "In this notebook, we will use both detection model and classification model with OpenVINO.We use [Object Detection Models](https://docs.openvino.ai/2020.2/usergroup3.html) and [Object Recognition Models](https://docs.openvino.ai/2020.2/usergroup4.html) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo).Detection model is used to detect vehicle position.Besides, we crop single vehicle and infer with classification model to recognize attributes of single vehicle.The pipline is here： \n",
    "<img align='center' src=\"data/vehicle-inference-flow.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "Finally, we will get the result:\n",
    "\n",
    "<img align='center' src=\"data/vehicle-result.png\" alt=\"drawing\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cae98b",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "We need import some basic package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.inference_engine import IECore\n",
    "from openvino.inference_engine.ie_api import ExecutableNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448e7312",
   "metadata": {},
   "source": [
    "# Download Models\n",
    "\n",
    "We need to download pretrained models to continue our progress.We use `omz_downloader`, a command-line tool installed by `openvino-dev` package.\n",
    "\n",
    "> Note: If you want to change the model,you need to modify the model name.If you want to change the precision,you need to modify the precision value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "# Model name as named in Open Model Zoo\n",
    "detection_model_name = \"vehicle-detection-0200\"\n",
    "recognition_model_name = \"vehicle-attributes-recognition-barrier-0039\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8)\n",
    "precision = \"FP32\"\n",
    "\n",
    "# Check the model exists \n",
    "detection_model_path = (\n",
    "    f\"model/intel/{detection_model_name}/{precision}/{detection_model_name}.xml\"\n",
    ")\n",
    "recognition_model_path = (\n",
    "    f\"model/intel/{recognition_model_name}/{precision}/{recognition_model_name}.xml\"\n",
    ")\n",
    "# Download detection model\n",
    "if not os.path.exists(detection_model_path):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                       f\"--name {detection_model_name} \" \\\n",
    "                       f\"--precision {precision} \" \\\n",
    "                       f\"--output_dir {base_model_dir}\"\n",
    "    ! $download_command\n",
    "# Download recognition model\n",
    "if not os.path.exists(recognition_model_path):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {recognition_model_name} \" \\\n",
    "                   f\"--precision {precision} \" \\\n",
    "                   f\"--output_dir {base_model_dir}\"\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c678fb",
   "metadata": {},
   "source": [
    "# Load Models\n",
    "\n",
    "In this notebook,we will need detection model and recognition model.When we download models,we need to initialize inference engine(IECore),and use `read_network` to read network architecture and weights from *.xml and *.bin files.Then,we load the network on the \"CPU\" using `load_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d874b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie_core = IECore()\n",
    "\n",
    "def model_init(model: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from file, load the\n",
    "    model on the CPU and get input and output names of nodes\n",
    "\n",
    "    :param: model: model architecture path *.xml\n",
    "    :retuns:\n",
    "            input_key: Input node network\n",
    "            output_key: Output node network\n",
    "            exec_net: Encoder model network\n",
    "            net: Model network\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the network and corresponding weights from file\n",
    "    net = ie_core.read_network(model)\n",
    "    # load the model on the CPU (you can use GPU or MYRIAD as well)\n",
    "    exec_net = ie_core.load_network(net, \"CPU\")\n",
    "    # Get input and output names of nodes\n",
    "    input_keys = list(exec_net.input_info)\n",
    "    output_keys = list(exec_net.outputs.keys())\n",
    "    return input_keys, output_keys, exec_net, net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c97641",
   "metadata": {},
   "source": [
    "### Get attributes from model\n",
    "\n",
    "We use `XXX.input_info.tensor_desc.dims` to get data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d864515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de -> detection\n",
    "# re -> recognition\n",
    "# Detection model initialization\n",
    "input_key_de, output_keys_de, exec_net_de, net_de = model_init(detection_model_path)\n",
    "# Recognition model initialization\n",
    "input_key_re, output_keys_re, exec_net_re, net_re = model_init(recognition_model_path)\n",
    "\n",
    "# Get input size - Detection\n",
    "height_de, width_de = net_de.input_info[input_key_de[0]].tensor_desc.dims[2:]\n",
    "# Get input size - Recognition\n",
    "height_re, width_re = net_re.input_info[input_key_re[0]].tensor_desc.dims[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb050ef3",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "\n",
    "1. `plt_show` function is used to show image \n",
    "2. `softmax` function is a generalization of the logistic function to multiple dimensions to normalize the output of a network to a probability distribution over predicted output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474826d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_show(raw_image):\n",
    "    \"\"\"\n",
    "    Use matplot to show image inline\n",
    "    raw_image: input image\n",
    "    \n",
    "    :param: raw_image:image array\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(raw_image)\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    x: np.array, axis=None\n",
    "    \"\"\"\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3595c8e4",
   "metadata": {},
   "source": [
    "### Read and show a test image\n",
    "\n",
    "From detection model input shape `[1, 3, 256, 256]`,we need to resize the image size to `256 x 256`,and expand batch channel with `expand_dims` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef702517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a image\n",
    "image_de = cv2.imread(\"data/cars.jpg\")\n",
    "# Resize to [3, 256, 256]\n",
    "resized_image_de = cv2.resize(image_de, (width_de, height_de))\n",
    "# Expand to [1, 3, 256, 256]\n",
    "input_image_de = np.expand_dims(resized_image_de.transpose(2, 0, 1), 0)\n",
    "# Show image\n",
    "plt_show(cv2.cvtColor(image_de, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6484b074",
   "metadata": {},
   "source": [
    "# Use detection network to detect vehicles\n",
    "\n",
    "![pipline](data/vehicle-inference-flow.png)\n",
    "\n",
    "In flowchart,We need single vehicles and send to recognition model.First,we use `infer` function to get the result.\n",
    "\n",
    "The detection model output has the format [image_id, label, conf, x_min, y_min, x_max, y_max], where:\n",
    "\n",
    "- image_id - ID of the image in the batch\n",
    "- label - predicted class ID (0 - vehicle)\n",
    "- conf - confidence for the predicted class\n",
    "- (x_min, y_min) - coordinates of the top left bounding box corner\n",
    "- (x_max, y_max) - coordinates of the bottom right bounding box corner\n",
    "\n",
    "We need to delete useless dims and filter out useless results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2540c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference the network\n",
    "result = exec_net_de.infer(inputs={input_key_de[0]: input_image_de})\n",
    "\n",
    "# Get the result\n",
    "boxes = result[next(iter(output_keys_de))]\n",
    "# delete the dim of 0, 1\n",
    "boxes = np.squeeze(boxes, (0, 1))\n",
    "# Remove zero only boxes\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c6523",
   "metadata": {},
   "source": [
    "### Detection Processing\n",
    "\n",
    "In this function, we use boxes to draw rectangles in image,and then we need to filter out low confidence results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27eeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use Detection model boxes to draw rectangles and plot the result\n",
    "    \n",
    "    :param: bgr_image: raw image\n",
    "    :param: resized_image: resized image\n",
    "    :param: boxes: detection model returns rectangle position\n",
    "    :param: threshold: confidence threshold\n",
    "    :returns: rgb_image: processed image\n",
    "    \"\"\"\n",
    "    # Define colors for boxes and descriptions\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch image shapes to calculate ratio\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert base image from bgr to rgb format\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    boxes = boxes[:, 2:]\n",
    "    # Iterate through non-zero boxes\n",
    "    for box in boxes:\n",
    "        # Pick confidence factor from last place in array\n",
    "        conf = box[0]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio\n",
    "            # In case that bounding box is found at the top of the image, \n",
    "            # we position upper box bar little bit lower to make it visible on image \n",
    "            # 在顶部找到的边界框，将box下移\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                int(max(corner_position * ratio_y * 256, 10)) if idx % 2 \n",
    "                else int(corner_position * ratio_x * 256)\n",
    "                for idx, corner_position in enumerate(box[1:]) # 不需要置信度\n",
    "            ]\n",
    "            \n",
    "            print(f\"x_min:{x_min}, y_min:{y_min}, x_max:{x_max}, y_max:{y_max}\")\n",
    "            \n",
    "            # Draw box based on position, parameters in rectangle function are: image, start_point, end_point, color, thickness\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"red\"], 2)\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ed237",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_show(convert_result_to_image(image_de, resized_image_de, boxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d5580",
   "metadata": {},
   "source": [
    "### Recognize the vehicle's attributes\n",
    "\n",
    "We choose one of the box,and crop the vehicle to test recognition model.Similarly,we need to resize the input image, and infer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbc9e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Crop the image with [y_min:y_max, x_min:x_max]\n",
    "# Three vehicles in car.jpg\n",
    "# test_car = resized_image_de[100:164, 0:100]\n",
    "# test_car = resized_image_de[130:215, 166:254]\n",
    "test_car = resized_image_de[92:152, 126:209]\n",
    "\n",
    "# resize image to input_size\n",
    "resized_image_re = cv2.resize(test_car, (width_re, height_re))\n",
    "input_image_re = np.expand_dims(resized_image_re.transpose(2, 0, 1), 0)\n",
    "plt_show(cv2.cvtColor(resized_image_re, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70376f",
   "metadata": {},
   "source": [
    "##### Recognition progressing\n",
    "\n",
    "We will get the result contains colors(White, gray, yellow, red, green, blue, black) and types(Car, bus, truck, van), then we need to get the probability of each attribute, finally we need to choose the max probability as the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2159ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vehicle_recognition(exec_net_re, input_size, raw_image):\n",
    "    \"\"\"\n",
    "    Vehicle attributes recognition, input a single vehicle, return it's attribute\n",
    "    :param: exec_net_re: recognition net \n",
    "    :param: input_size: recognition input size\n",
    "    :param: raw_image: single vehicle image\n",
    "    :returns: attr_color: predicted color\n",
    "              attr_type: predicted type\n",
    "    \"\"\"\n",
    "    # vehicle's attribute\n",
    "    colors = ['White', 'Gray', 'Yellow', 'Red', 'Green', 'Blue', 'Black']\n",
    "    types = ['Car', 'Bus', 'Truck', 'Van']\n",
    "    \n",
    "    # resize image to input size\n",
    "    resized_image_re = cv2.resize(raw_image, input_size)\n",
    "    input_image_re = np.expand_dims(resized_image_re.transpose(2, 0, 1), 0)\n",
    "    plt_show(cv2.cvtColor(resized_image_re, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    # inference the recognition net\n",
    "    result = exec_net_re.infer(inputs={input_key_re[0]: input_image_re})\n",
    "    \n",
    "    # predict result\n",
    "    predict_colors = result['color']\n",
    "    # delete the dim of 2, 3\n",
    "    predict_colors = np.squeeze(predict_colors, (2, 3))\n",
    "    predict_types = result['type']\n",
    "    predict_types = np.squeeze(predict_types, (2, 3))\n",
    "\n",
    "    attr_color, attr_type = (colors[np.argmax(softmax(predict_colors))],\n",
    "                             types[np.argmax(softmax(predict_types))])\n",
    "    return attr_color, attr_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79505e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Attributes:{vehicle_recognition(exec_net_re, (72, 72), test_car)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54075f4e",
   "metadata": {},
   "source": [
    "### Conbine two models\n",
    "\n",
    "Congratulations!Now we can use detection model to crop single vehicle and recognize the vehicle's attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59402e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_result_to_image(exec_net_re, bgr_image, resized_image, boxes, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Use Detection model boxes to draw rectangles and plot the result\n",
    "    \n",
    "    :param: bgr_image: raw image\n",
    "    :param: resized_image: resized image\n",
    "    :param: boxes: detection model returns rectangle position\n",
    "    :param: threshold: confidence threshold\n",
    "    :returns: rgb_image: processed image\n",
    "    \"\"\"\n",
    "    # Define colors for boxes and descriptions\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch image shapes to calculate ratio\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert base image from bgr to rgb format\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # original boxes are: image_id, label, conf, x_min, y_min, x_max, y_max\n",
    "    boxes = boxes[:, 2:]\n",
    "    # Iterate through non-zero boxes\n",
    "    for box in boxes:\n",
    "        # Pick confidence factor from first place in array\n",
    "        conf = box[0]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio\n",
    "            # Besides, we need to multiply the position scale ratio \n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                int(max(corner_position * ratio_y * 256, 10)) if idx % 2 \n",
    "                else int(corner_position * ratio_x * 256)\n",
    "                for idx, corner_position in enumerate(box[1:]) \n",
    "            ]\n",
    "            \n",
    "            # Do vehicle recognition inference\n",
    "            attr_color, attr_type = vehicle_recognition(exec_net_re, (72, 72), \n",
    "                                                        resized_image[y_min:y_max, x_min:x_max])\n",
    "            \n",
    "            # close the vehicle window\n",
    "            plt.close()\n",
    "            \n",
    "            # Draw box based on position\n",
    "            # Parameters in rectangle function are: image, start_point, end_point, color, thickness\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"red\"], 2)\n",
    "            \n",
    "            # Draw vehicle attribute \n",
    "            # parameters in putText function are: img, text, org, fontFace, fontScale, color, thickness, lineType\n",
    "            rgb_image = cv2.putText(rgb_image, \n",
    "                                    f\"A {attr_color} {attr_type}\",\n",
    "                                   (x_min, y_min - 10),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                   0.5,\n",
    "                                   colors[\"green\"],\n",
    "                                   1,\n",
    "                                   cv2.LINE_AA)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_show(convert_result_to_image(exec_net_re, image_de, resized_image_de, boxes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
